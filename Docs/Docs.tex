\documentclass[14pt]{extarticle}

\usepackage[top=2cm, bottom=2cm, right=2cm, left=2cm]{geometry}

\usepackage{caption}
\usepackage[table]{xcolor}

\usepackage{array}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}

\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\setlength\parindent{0pt}

\usepackage{graphicx}
\usepackage{float}


\title{An Implementation of ``Adversarial Discriminative Domain Adaptation"}

\author{Ali Jafari, Behrouz Ghamkhar}

\begin{document}
	\pagenumbering{gobble}
	\maketitle
	\newpage
	\tableofcontents
	\newpage
	\pagenumbering{arabic}
	\section{Introduction}
		While methods based on deep learning have achieved state-of-the-art results on numerous tasks, most of these methods are based on the assumption that the data used to train the model is drawn from the same distribution that the data used to test the model is drawn from. This assumption can be violated in the real world, where AI models should be able to adapt themselves to new environments with little human supervision. In order to address this problem, methods based on \textit{single-source unsupervised domain adaptation} utilize labeled data from a \textit{source domain} to achieve satisfactory performance on unlabeled data from a different but related \textit{target domain} \cite{udasurvey}.\\\\
		In this document, we discuss our implementation of ``Adversarial Discriminative Domain Adaptation" \cite{adda}, which is based on PyTorch. In addition to documenting our implementation of the paper, we provide the results of our replications of the performed experiments. Furthermore, we comment on the result of each replicated experiment and offer possible reasons for the success or failure of every experiment.
		\subsection{Overview of the Method}
		In \cite{adda}, the authors propose a framework for single-source unsupervised domain adaptation on a classification task that is trained in three stages. The assumptions of the method are those of single-source unsupervised domain adaptation presented earlier in the introduction, as well as the assumption that the source domain and the target domain share the same set of classes.\\\\
		Firstly, a \textit{source encoder} and a \textit{classifier} are jointly trained on labeled data from the source domain. Secondly, a \textit{target encoder} is trained adversarially against a \textit{discriminator}. This is performed in a way that given unlabeled data from the target domain, the target encoder can generate representations similar to the representations generated by the source encoder, given data from the source domain. In this sense, the second stage of training in the framework proposed by \cite{adda} can be understood like the training of a Generative Adversarial Network \cite{gan}. In this analogy, the target encoder is the generator and the `real data' that the target encoder is trying to forge consists of representations generated by the source encoder from data belonging to the source domain. At the beginning of the second stage of training, the weights of the target encoder are initialized from the weights of the source encoder trained on the source domain in the first stage. During the second stage, the weights of the source encoder are fixed, and only the target encoder and the discriminator are trained. Finally, the weights of the target encoder trained in the second stage and the classifier trained in the first stage are fixed. Given unlabeled data from the target domain, the target encoder and the classifier attempt in conjunction to predict class labels for the unlabeled target data \cite{adda}. The intuitive justification behind this framework is that if the source encoder and the classifier are trained well on the source domain in the first stage, then given representations generated by the source encoder from data belonging to the source domain, the classifier can accurately predict class labels for data from the source domain. Consequently, if the target encoder can receive data from the target domain and generate similar representations to those that the source encoder can create from source data, then we can give the representations generated by the target encoder to the classifier and expect a reasonably accurate classification performance on the unlabeled target data. Figure 1 illustrates the three stages of the framework.
		\begin{figure}[H]
			\begin{center}
				\fbox{\includegraphics[scale=0.55]{adda.jpg}}
				\caption{The framework proposed in [2], in which dashed lines around a network indicate that the weights of that network are fixed. This figure belongs to [2].}
			\end{center}
		\end{figure}
		\subsection{Outline of the Document}
		The rest of this document is organized as follows: Firstly, we explain how our implementation is structured and what the role of each module is. We attempt to follow a `top-down' approach in explaining the implementation. That is, we begin by discussing the more important parts of the code and then focus on the other modules. Following an explanation of the implementation, we report the results of our replications of the experiments performed in \cite{adda}. As mentioned earlier, we aim to offer insights into possible reasons behind the success or failure of each replicated experiment. In addition, we should note that since we have implemented \cite{adda} concretely, and since the relevant mathematical content is present in \cite{adda} as well, we focus on an intuitive understanding of the learning framework proposed in \cite{adda} throughout this document. Throughout this document, we let S $\rightarrow$ T denote an experiment in which S is the source domain and T is the target domain.
		\section{The Implementation}
		Before discussing the low-level details of the implementation, we describe the structure of the project and the role of each module.
		\subsection{Overview of the Implementation}
		On the highest level, our implementation consists of six modules:
		\begin{enumerate}
			\item \texttt{Training}: This module contains three classes, each implementing a single stage of the framework proposed in \cite{adda}. The three classes are called \texttt{SourceTrainer}, \texttt{Adapter}, and \texttt{NetworkTester}. The \texttt{SourceTrainer} class implements the first stage of the learning framework, while the \texttt{Adapter} and \texttt{NetworkTester} classes implement the second and third stages, respectively.
			\item \texttt{Experiments}: This module contains a single class called \texttt{Experiment}, which uses the three classes provided in the \texttt{Training} module to run the experiments described in \cite{adda}.
			\item \texttt{Networks}: This module contains implementations for some of the neural networks that are used in the experiments of \cite{adda}. It consists of \texttt{Encoders}, \texttt{Discriminators} and \texttt{Classifiers}. We should note that since classifiers of some experiments only consisted of a simple \texttt{Linear} layer (as defined in PyTorch), we defined those classifiers directly in the aforementioned experiments.
			\item \texttt{Extractors}: This module is used to preprocess different datasets and save them in a compressed format so that entire datasets can be loaded fast and used within the context of PyTorch. The \texttt{Extractors} module contains a single concrete class only, called \texttt{LabeledDatasetExtractor}. This class works on the assumption that the original dataset is labeled and consists of a number of folders, one for each class, and that each element of the dataset exists as a file in the folder of the class it belongs to. Since all the datasets used in \cite{adda} are labeled, the \texttt{LabeledDatasetExtractor} class is sufficient for preprocessing all the datasets examined in \cite{adda}. However, similar to the experiments performed in \cite{adda}, the labels of datasets representing target domains are not used in any of the experiments.
			\item \texttt{Processors}: The \texttt{LabeledDatasetExtractor} class relies on a \textit{processor} to transform a single element of a dataset into an intended format. Therefore, it uses a processor on each element of a dataset it extracts. Since all the datasets used in \cite{adda} consist of images, the \texttt{Processors} module contains a single concrete class called \texttt{ImageProcessor}. This class normalizes an image into the range $[0, 255]$. The \texttt{ImageProcessor} class also allows resizing an image to an intended size, as well as converting an image to grayscale.
			\item \texttt{Datasets}: This module is used for loading entire datasets extracted by an extractor. Since all the datasets used in \cite{adda} are labeled, the \texttt{Datasets} module contains a single class. This class is called \texttt{LabeledDataset} and it follows the conventions of writing datasets in PyTorch.
		\end{enumerate}
		In addition, the project contains seven \texttt{.py} files, one for each of the experiments performed in \cite{adda}. Each of the aforementioned files (\texttt{mnist\_usps.py}, \texttt{usps\_mnist.py}, \texttt{svhn\_mnist.py}, \texttt{modality\_adaptation.py}, \texttt{amazon\_webcam.py}, \texttt{dslr\_webcam.py}, \texttt{webcam\_dslr.py}) uses the \texttt{Experiment} class to run the experiment indicated in its name. Next, we discuss each of the modules described earlier in finer detail.
		\subsection{The Training Module}
		As mentioned earlier, the \texttt{Training} module consists of three classes. These classes are called \texttt{SourceTrainer}, \texttt{Adapter}, and \texttt{NetworkTester}. Each class implements a single stage of the learning framework proposed in \cite{adda}. Here, we describe the details of these classes.
		\subsubsection{The SourceTrainer Class}
		This class implements the first stage of the learning framework described in \cite{adda}. The constructor of this class receives everything needed to perform the first stage of the aforementioned learning framework. This includes a source encoder, a classifier, a source dataset, a criterion (loss function), an optimizer, the number of iterations used in this stage of the learning framework, the batch size used in this stage of the learning framework, and the device that the source encoder and classifier are trained on (for example, CPU or GPU). Note that since both the source encoder and the classifier should be trained, the optimizer received in the constructor of this class updates the parameters of both the source encoder and the classifier. The \texttt{train} method of this class performs the intended supervised learning on the source domain. This method follows the conventional way through which supervised learning is implemented using PyTorch, using a PyTorch \texttt{DataLoader} to perform mini-batch gradient descent. In order to follow the procedure described in \cite{adda}, we always used the categorical cross-entropy loss function as the criterion attribute of this class.
		\subsubsection{The Adapter Class}
		This class implements the second stage of the learning framework described in \cite{adda}. The constructor of this class receives a source encoder, a target encoder, and a discriminator. In addition, the constructor receives a source dataset, a target dataset, optimizers for the discriminator and the target encoder, the number of iterations used in this stage of the learning framework, the batch size used in this stage of the learning framework, the device used in this stage of the learning framework, and a classifier. We note that based on \cite{adda}, a classifier is not necessary for this stage. However, we used the classifier trained by the \texttt{SourceTrainer} class to monitor the performance of the target encoder on the unlabeled target data during this stage of the learning framework. Another detail to note is that, unlike \texttt{SourceTrainer}, the constructor of \texttt{Adapter} does not receive a loss function. This is because the adversarial training described in \cite{adda} always uses a binary cross-entropy loss function.\\\\
		The \texttt{Adapter} class contains three other methods: \texttt{adapt}, \texttt{train\_discriminator}, and \texttt{train\_target\_encoder}. The \texttt{adapt} method runs a loop for the number of iterations received in the class constructor. In each iteration of this loop, \texttt{train\_discriminator} and then \texttt{train\_target\_encoder} are called. This is a high-level implementation of Algorithm 1 described in \cite{gan}.\\\\
		The \texttt{train\_discriminator} method implements one iteration of training the discriminator, as described in Algorithm 1 of \cite{gan} and adapted by \cite{adda}. In order to do so, we begin by sampling a batch of data from the source dataset and giving it to the source encoder. Then, we detach the representations generated by the source encoder to prevent an unwanted parameter update of the source encoder. Subsequently, we give the aforementioned representations to the discriminator as `real data', and ask the discriminator to predict whether its input data is `real' (coming from the source encoder) or `fake' (coming from the target encoder). Next, we sample a batch of data from the target dataset and give it to the target encoder. Then, we detach the representations generated by the target encoder to prevent an unwanted parameter update of the target encoder. Afterwards, we give the aforementioned representations to the discriminator as `fake data', and ask the discriminator to predict whether its input data is `real' or `fake'. Finally, we update the parameters of the discriminator using a binary cross-entropy loss function, based on its prediction error.\\\\
		The \texttt{train\_target\_encoder} method implements one iteration of training the target encoder in the second stage of the learning framework proposed by \cite{adda}. As mentioned earlier, this is similar to how the generator is trained in Algorithm 1 of \cite{gan}. In order to do so, we begin by sampling a batch of data from the target dataset and giving it to the target encoder. Since the target encoder has to be updated in this method, we do not detach the representations generated by the target encoder. Next, we give the aforementioned representations to the discriminator as `real data'. Finally, we update the parameters of the target encoder using a binary cross-entropy loss function, which is based on the prediction error of the discriminator. That is, the target encoder will have a higher loss, if and only if the discriminator has more success in correctly detecting that the representations are generated by the target encoder and not by the source encoder.
		\subsubsection{The NetworkTester Class}
		This class implements the third stage of the learning framework proposed in \cite{adda}. The constructor of this class receives an encoder, a classifier, a dataset, a batch size, and a device. The \texttt{test} method of this class computes the classification accuracy of the received encoder on the received dataset. It counts the total number of elements present in the dataset, as well as those which are classified correctly by the aforementioned encoder. In addition, it counts the number of elements belonging to each class in the dataset, as well as the number of elements in each class that the encoder has correctly classified. Finally, the \texttt{test} method computes the classification accuracy of the encoder on the entire dataset, as well as the classification accuracy of the encoder on each class in the dataset. We should note that even though the \texttt{NetworkTester} class is primarily used for implementing the third stage of the learning framework illustrated in \cite{adda}, we  also used this class for monitoring other classification accuracies throughout the training process. For example, we mentioned earlier that in the \texttt{Adapter} class, we monitored the accuracy of the target encoder on the unlabeled target data. In order to do so, we used the \texttt{NetworkTester} class.
		\subsection{The Experiments Module}
		As stated earlier, this module consists of a single class called \texttt{Experiment}. We used this class to run the experiments mentioned in \cite{adda}. In order to do so, the Experiment class uses the three classes in the Training module.
		\subsubsection{The Experiment Class}
		The constructor of this class receives everything needed to initialize objects of \texttt{SourceTrainer}, \texttt{Adapter} and \texttt{NetworkTester} classes, as explained earlier. The \texttt{run} method of this class implements the complete learning framework illustrated in \cite{adda}. As mentioned earlier, the learning framework relies on the assumption that the source encoder and the classifier are trained well on the source domain. Therefore, we used \texttt{NetworkTester} to measure their performance on the source domain. In order to compare against the \textit{source only} evaluation metric reported in \cite{adda}, we also used \texttt{NetworkTester} to measure the performance of the source encoder and the classifier on the target domain. Note that similar to \cite{adda}, we initialized the parameters of the target encoder with those of the source encoder after the first stage of training.
		\subsection{The Networks Module}
		Earlier, we stated that this module consists of \texttt{Encoders}, \texttt{Discriminators} and \texttt{Classifiers}. Here, we explain our method behind providing an implementation of each.
		\subsubsection{Encoders}
		The encoders consist of \texttt{LeNetEncoder}, \texttt{VGG16Encoder}, \texttt{ResNet50Encoder} and \texttt{ResNet18Encoder}. There is also a class called \texttt{Identity}, which implements an identity function as a PyTorch network. \texttt{Identity} separates the encoder of a pre-trained PyTorch model from its classifier, since we needed separate encoder and classifier networks to replicate the learning framework of \cite{adda}. Similar to \cite{adda}, we adapted the variant of LeNet implemented in the source code of Caffe \cite{CaffeLeNet}. However, we excluded the last linear layer of the network in our adapted implementation and created it directly as the classifier network in the relevant experiments instead.\\\\
		For \texttt{VGG16Encoder}, \texttt{ResNet50Encoder} and \texttt{ResNet18Encoder}, we used the pre-trained networks provided in PyTorch. For the reason mentioned in the previous paragraph, we replaced the classifiers of the pre-trained networks with identity functions. We note that even though none of the experiments in \cite{adda} use ResNet18, we had to use it because limitations in computational resources prevented us from using VGG16 or ResNet50 in the relevant experiments. Although we still chose to publish their classes. Also, we fixed the parameters in the last convolutional layer of \texttt{ResNet18Encoder}, for the reason mentioned in the \textit{office} experiment of \cite{adda}.
		\subsubsection{Discriminators}
		There are three different discriminators described in the experiments of \cite{adda}. Consequently, we have \texttt{DigitsDiscriminator},  \texttt{ModalityAdaptationDiscriminator} and \texttt{OfficeDiscriminator} as our discriminator networks. We named each discriminator after the experiment in \cite{adda} it belongs to and implemented each as described in \cite{adda}.
		\subsubsection{Classifiers}
		As stated earlier, we separated encoders from classifiers. Classifiers belonging to architectures other than VGG16 were single linear layers that were used directly in experiments as needed. For VGG16, we implemented the \texttt{VGG16Classifier} class. This class loads the pre-trained VGG16 network available in PyTorch and only keeps the classifier attribute.
		\subsection{The Extractors Module}
		As explained earlier, we used this module to preprocess different datasets and save them in a compressed format. This module contains a single concrete class, called \texttt{LabeledDatasetExtractor}. However, \texttt{LabeledDatasetExtractor} inherits from an abstract class called \texttt{Extractor}. We did so to give the code more flexibility and provide the possibility of adding further extractors, in case one needed to extend this project.
		\subsubsection{The LabeledDatasetExtractor Class}
		The \texttt{LabeledDatasetExtractor} class assumes that the input dataset is labeled and consists of a number of folders, one for each class in the dataset, and that each element of the dataset exists as a file in the folder of the class in the dataset it belongs to. In addition, the \texttt{LabeledDatasetExtractor} class assumes that each element of the dataset should ultimately have the same feature shape. For instance, if the dataset consists of images, the \texttt{LabeledDatasetExtractor} class assumes that each image should ultimately have the same size and number of channels. This class has two methods other than the constructor: \texttt{extract} and \texttt{save}. The \texttt{extract} method of this class traverses the folders belonging to the classes of the dataset in order and retrieves each element of the dataset. Furthermore, this method uses a \textit{processor} to transform each element of the input dataset into an intended format. Additionally, the \texttt{extract} method assigns a numerical class label to the members of each class in the dataset, starting from zero and incrementing by one for each new class. Finally, this method uses the \texttt{class\_dict} attribute to keep track of the corresponding class name of each numerical class label. The \texttt{save} method of \texttt{LabeledDatasetExtractor} stores the preprocessed dataset in a location. It creates a \texttt{.npz} file to store the elements of the dataset and their numerical labels. Additionally, it creates a \texttt{.pkl} file to store the \texttt{class\_dict} attribute. We should note that we stored all of our preprocessed datasets in a folder called \texttt{StoredDatasets} (as is evident in our \texttt{.gitignore} file). However, due to the larger size of this folder compared to the rest of the project, we decided not to upload it to the project's GitHub repository. Nevertheless, if other people desire to test the code, all datasets used in \cite{adda} can be extracted with the \texttt{LabeledDatasetExtractor} class. Some of them already existed in a format to be used by this class \cite{mnistjpg}, \cite{officepwc}. We also converted the rest of the datasets to the aforementioned format \cite{uspsjpg}, \cite{svhnjpg}, \cite{nyud2jpg}.
		\subsection{The Processors Module}
		As mentioned earlier, the \texttt{LabeledDatasetExtractor} class relies on a processor to transform a single element of a dataset into an intended format. This module consists of a single concrete class called \texttt{ImageProcessor}. However, \texttt{ImageProcessor} inherits from an abstract class called \texttt{Processor}. Our intention behind this decision as well was to make the code suitable for possible extensions in the future.
		\subsubsection{The ImageProcessor Class}
		This class normalizes images into the range $[0, 255]$, resizes an image to an intended \textit{target size}, and can convert an image to grayscale. The constructor of this class only receives a boolean value that determines whether the instantiated object of \texttt{ImageProcessor} should convert images to grayscale or not. The default value of this boolean variable is false. Other than the constructor, this class has two methods: \texttt{set\_target\_size} and \texttt{process}. The \texttt{set\_target\_size} method assigns the intended size to which the instance of \texttt{ImageProcessor} should resize images. This method is called in the constructor of \texttt{LabeledDatasetExtractor}, based on the feature shape that the instance of \texttt{LabeledDatasetExtractor} has received. The \texttt{process} method of this class loads an image from disk, resizes it to the intended target size, converts the image to grayscale if desired, normalizes the image to the range $[0, 255]$ and returns the resulting image.
		\subsection{The Datasets Module}
		Earlier, we mentioned that we used this module for loading entire datasets extracted by an extractor. Because of the reasons explained previously, this module contains a single class called \texttt{LabeledDataset}.
		\subsubsection{The LabeledDataset Class}
		This class loads a dataset extracted by \texttt{LabeledDatasetExtractor} entirely into memory. The constructor of this class receives the path to the folder that contains the extracted dataset and the name of the dataset. In addition, it receives a transform parameter and a sample size parameter. The transform parameter indicates a transform or a list of transforms that can be applied to each element in the extracted dataset. This is similar to how the default datasets available in PyTorch accept transforms. The sample size parameter is used for randomly sampling a subset of the extracted dataset instead of using the entire dataset. We created this feature since \cite{adda} only uses a randomly-sampled subset of the MNIST \cite{mnist} and USPS \cite{usps} datasets in the MNIST $\rightarrow$ USPS and USPS $\rightarrow$ MNIST experiments. Furthermore, the constructor of \texttt{LabeledDataset} loads an extracted dataset and the numerical class labels in the extracted dataset using the given path and dataset name. Additionally, it loads the name associated with each numerical class label in the extracted dataset. The rest of the implementation of \texttt{LabeledDataset} follows the conventions of writing datasets in PyTorch.
		\section{Replicated Experiments}
		We replicated all seven experiments in \cite{adda}. The datasets used in the experiments of \cite{adda} are MNIST, USPS, SVHN \cite{svhn}, NYU Depth \cite{nyud2} and Office-31 \cite{office31}.\\\\
		The first three experiments in \cite{adda} are performed on the MNIST, USPS and SVHN datasets. These three datasets consist of digits 0 - 9 spread across ten classes. Specifically, these experiments are MNIST $\rightarrow$ USPS, USPS $\rightarrow$ MNIST and SVHN $\rightarrow$ MNIST.\\\\
		The fourth experiment of \cite{adda} is performed on a subset of nineteen classes from the labeled portion of the NYU Depth dataset. This dataset consists of RGB and depth images captured in several indoor scenes. Specifically, this experiment in \cite{adda} is performed by choosing a subset of the RGB images in the dataset as the source domain and a different subset of the depth images as the target domain.\\\\
		The rest of the experiments in \cite{adda} are performed on the Office-31 dataset. This dataset contains three different domains, each having the same thirty-one classes. The three domains in this dataset are \textit{amazon}, \textit{dslr} and \textit{webcam}. The images in the \textit{amazon} domain are downloaded from \texttt{www.amazon.com}, while the images in the \textit{dslr} and \textit{webcam} domains are captured using a high-definition camera and a low-definition camera, respectively. The experiments performed on the Office-31 dataset in \cite{adda} are amazon $\rightarrow$ webcam, dslr $\rightarrow$ webcam and webcam $\rightarrow$ dslr.\\\\
		We ran each of the first three experiments ten times. Due to limited computational resources, we were able to run each of the other experiments three times. We report the mean classification accuracy, plus or minus the standard deviation of classification accuracy for each experiment. In tables referring to reported results, `Source Only' refers to the classification accuracy of the source encoder and the classifier on the target domain right after completing the first stage of the learning framework. `Adaptation' refers to the classification accuracy of the target encoder and the classifier on the target domain after completing the entire learning procedure.
		\subsection{The MNIST $\rightarrow$ USPS Experiment}
		We implemented this experiment in \texttt{mnist\_usps.py}. Similar to \cite{adda}, we used modified LeNet networks as our source and target encoders. Furthermore, we ran each of the first two stages of the learning framework for 10000 iterations using a batch size of 128. For the first stage, we used the Adam optimizer with a learning rate of 0.001. For the second stage, we used the Adam optimizer with a learning rate of 0.0002, $\beta_1 = 0.5$ and $\beta_2 = 0.999$. We converted all images to grayscale and resized each to $28 \times 28$. Also, following \cite{adda}, we only used 2000 randomly-sampled elements of MNIST as our source domain and 1800 randomly-sampled elements of USPS as our target domain in each experiment. Table 1 shows the results of this replicated experiment.
		\begin{table}[H]
			\centering
			\begin{tabular}{|P{40mm}|P{40mm}|P{40mm}|}
				\hline
				\cellcolor{gray!50} & Source Only (\%) & Adaptation (\%)\\
				\hline
				Ours & $65.31 \pm 3.17$ & $88.73 \pm 3.51$\\
				\hline
				\cite{adda} & $75.2 \pm 1.6$ & $89.4 \pm 0.2$\\
				\hline
			\end{tabular}
			\caption{Comparison of results on MNIST $\rightarrow$ USPS}
		\end{table}
		Our replication of MNIST $\rightarrow$ USPS yields similar results to those of \cite{adda}. Additionally, because of the significant improvement in the classification accuracy on the target domain after the adaptation stage, we conclude that the learning framework of \cite{adda} effectively solves this task.
		\subsection{The USPS $\rightarrow$ MNIST Experiment}
		We implemented this experiment in \texttt{usps\_mnist.py}. Following \cite{adda}, other conditions of this experiment were exactly like those of MNIST $\rightarrow$ USPS. Table 2 shows the results of this replicated experiment.
		\begin{table}[H]
			\centering
			\begin{tabular}{|P{40mm}|P{40mm}|P{40mm}|}
				\hline
				\cellcolor{gray!50} & Source Only (\%) & Adaptation (\%)\\
				\hline
				Ours & $41.19 \pm 3.11$ & $84.47 \pm 2.39$\\
				\hline
				\cite{adda} & $57.1 \pm 1.7$ & $90.1 \pm 0.8$\\
				\hline
			\end{tabular}
			\caption{Comparison of results on USPS $\rightarrow$ MNIST}
		\end{table}
		We observe that our replication of USPS $\rightarrow$ MNIST also yields similar results to those of \cite{adda}. Additionally, because of the significant improvement in the classification accuracy on the target domain after the adaptation stage, we conclude that the learning framework of \cite{adda} is effective in solving this task as well.
		\subsection{The SVHN $\rightarrow$ MNIST Experiment}
		We implemented this experiment in \texttt{svhn\_mnist.py}. Following \cite{adda}, other conditions of this experiment were exactly like those of MNIST $\rightarrow$ USPS, except for the dataset sizes. Table 3 shows the results of this replicated experiment.
		\begin{table}[H]
			\centering
			\begin{tabular}{|P{40mm}|P{40mm}|P{40mm}|}
				\hline
				\cellcolor{gray!50} & Source Only (\%) & Adaptation (\%)\\
				\hline
				Ours & $58.54 \pm 2.77$ & $32.63 \pm 2.43$\\
				\hline
				\cite{adda} & $60.1 \pm 1.1$ & $76.0 \pm 1.8$\\
				\hline
			\end{tabular}
			\caption{Comparison of results on SVHN $\rightarrow$ MNIST}
		\end{table}
		Our replication of SVHN $\rightarrow$ MNIST does not yield similar results to those of \cite{adda}. Since our `Source Only' classification accuracy is already much higher than our `Adaptation' classification accuracy, we can consider attributing the aforementioned discrepancy to a problem in the adaptation stage. Nowadays, the traditional way of adversarial training is known to be difficult to converge. We propose that building the adaptation stage on more recent adversarial training methods such as WGANs \cite{wgan} might be a potential solution to this problem. Also, since the classification accuracy of the source encoder and the classifier on the source domain was $98.58 \pm 0.14$ percent, we conclude that our preprocessing of the SVHN dataset is not a primary cause of the current problem.
		\subsection{The Modality Adaptation Experiment}
		We implemented this experiment in \texttt{modality\_adaptation.py}. Following \cite{adda}, we only considered 19 classes of the NYU Depth dataset and ignored the rest. That is, we only cropped objects belonging to the following classes: bathtub, bed, bookshelf, box, chair, counter, desk, door, dresser, garbage bin, lamp, monitor, night stand, pillow, sink, sofa, table, television and toilet. Our code for preprocessing this dataset, as described in \cite{adda}, can be found on \cite{nyud2jpg}. However, we should note that the official train/test split of the NYU Depth dataset that we could find \cite{nyud2split} was different from the split described in \cite{adda}. Also, we did not use HHA encoding on the depth maps. Instead, the three channels of each encoded depth map are duplicates of the original single-channel depth map. As stated earlier, due to limited computational resources, we used a ResNet18 architecture instead of VGG16 for this experiment. Furthermore, we ran each of the first two stages of the learning framework for 20000 iterations using a batch size of 128. For the first stage, we used the Adam optimizer with a learning rate of 0.001. For the second stage, we used the Adam optimizer with a learning rate of 0.0002, $\beta_1 = 0.5$ and $\beta_2 = 0.999$. We ran this experiment on Google Colab. Table 4 shows the results of this replicated experiment.
		\begin{table}[H]
			\centering
			\begin{tabular}{|P{40mm}|P{40mm}|P{40mm}|}
				\hline
				\cellcolor{gray!50} & Source Only (\%) & Adaptation (\%)\\
				\hline
				Ours & $9.32 \pm 0.24$ & $15.32 \pm 1.40$\\
				\hline
				\cite{adda} & $13.9$ & $21.1$\\
				\hline
			\end{tabular}
			\caption{Comparison of results on the modality adaptation experiment}
		\end{table}
		Our replication of the modality adaptation experiment yields similar results to those of \cite{adda}, despite the fact that we did not use HHA encoding on the depth maps and also used a simpler architecture for our encoders. We have several reasons to think so. Firstly, the difference between our `Adaptation' and `Source Only' classification accuracies is similar to that of \cite{adda}. Moreover, unlike other experiments, \cite{adda} does not report the results of other unsupervised domain adaptation methods in this experiment. Furthermore, unlike previous experiments, \cite{adda} does not include the standard deviation of the classification accuracies in this experiment. Finally, \cite{adda} reports that a classifier trained directly on the target domain yields an average classification accuracy of 46.8 percent, implying that the dataset chosen and preprocessed specifically as described is not so suitable for the task at hand.
		\subsection{The Amazon $\rightarrow$ Webcam Experiment}
		We implemented this experiment in \texttt{amazon\_webcam.py}. Following \cite{adda}, for the first stage of the learning framework, we used the SGD optimizer with a learning rate of 0.001 and a momentum of 0.9. For the second stage, we used the Adam optimizer with a learning rate of 0.0002, $\beta_1 = 0.5$ and $\beta_2 = 0.999$. We ran both stages for 20000 iterations with a batch size of 64. However, for reasons explained earlier, we used a ResNet18 architecture instead of ResNet50. We ran this experiment on Google Colab. Table 5 shows the results of this replicated experiment.
		\begin{table}[H]
			\centering
			\begin{tabular}{|P{40mm}|P{40mm}|P{40mm}|}
				\hline
				\cellcolor{gray!50} & Source Only (\%) & Adaptation (\%)\\
				\hline
				Ours & $23.77 \pm 0.43$ & $55.0 \pm 2.94$\\
				\hline
				\cite{adda} & $64.2$ & $75.1$\\
				\hline
			\end{tabular}
			\caption{Comparison of results on Amazon $\rightarrow$ Webcam}
		\end{table}
		Our replication of amazon $\rightarrow$ webcam does not yield similar results to those of \cite{adda}. However, the difference between our `Source Only' and `Adaptation' classification accuracies in this experiment is still high, even much higher than that of \cite{adda}. Also, our `Source Only' classification accuracy is much lower than that of \cite{adda}. This implies that our adaptation step was performed correctly. However, we suspect that our different results overall are due to the fact that we used a simpler architecture for our encoders.
		\subsection{The DSLR $\rightarrow$ Webcam Experiment}
		We implemented this experiment in \texttt{dslr\_webcam.py}. Following \cite{adda}, we used the same hyperparameters as those in the amazon $\rightarrow$ webcam experiment. However, for reasons explained earlier, we used a ResNet18 architecture instead of ResNet50. We ran this experiment on Google Colab. Table 6 shows the results of this replicated experiment.
		\begin{table}[H]
			\centering
			\begin{tabular}{|P{40mm}|P{40mm}|P{40mm}|}
				\hline
				\cellcolor{gray!50} & Source Only (\%) & Adaptation (\%)\\
				\hline
				Ours & $61.71 \pm 0.82$ & $86.92 \pm 0.62$\\
				\hline
				\cite{adda} & $96.1$ & $97.0$\\
				\hline
			\end{tabular}
			\caption{Comparison of results on DSLR $\rightarrow$ Webcam}
		\end{table}
		We observe that our replication of dslr $\rightarrow$ webcam yields comparable results to those of \cite{adda}. Specifically, the difference between our `Source Only' and `Adaptation' classification accuracies in this experiment is very high, implying that the adaptation step has a considerable impact on the ultimate classification accuracy. Furthermore, the `Source Only' classification accuracy of this experiment reported in \cite{adda} is already very high. This leads us to believe that our utilization of a simpler architecture had some negative impact on our final classification accuracy.
		\subsection{The Webcam $\rightarrow$ DSLR Experiment}
		We implemented this experiment in \texttt{webcam\_dslr.py}. Following \cite{adda}, we used the same hyperparameters as those in the amazon $\rightarrow$ webcam experiment. However, for reasons explained earlier, we used a ResNet18 architecture instead of ResNet50. We ran this experiment on Google Colab. Table 7 shows the results of this replicated experiment.
		\begin{table}[H]
			\centering
			\begin{tabular}{|P{40mm}|P{40mm}|P{40mm}|}
				\hline
				\cellcolor{gray!50} & Source Only (\%) & Adaptation (\%)\\
				\hline
				Ours & $82.2 \pm 1.23$ & $87.75 \pm 0.48$\\
				\hline
				\cite{adda} & $97.8$ & $99.6$\\
				\hline
			\end{tabular}
			\caption{Comparison of results on Webcam $\rightarrow$ DSLR}
		\end{table}
		We observe that our replication of webcam $\rightarrow$ dslr yields comparable results to those of \cite{adda}. Specifically, there is some difference between our `Source Only' and `Adaptation' classification accuracies in this experiment, implying that the adaptation step had positively impacted the ultimate classification accuracy. Furthermore, the `Source Only' classification accuracy of this experiment reported in \cite{adda} is already very high. This leads us to believe that our utilization of a simpler architecture had negatively impacted our final classification accuracy.
		\section{Conclusion and Future Work}
		In this document, we described our implementation of ``Adversarial Discriminative Domain Adaptation" \cite{adda}, which attempts to use labeled data from a source domain to achieve remarkable performance on unlabeled data from a different but related target domain. We discussed the results of our replications of all seven experiments in \cite{adda} and attempted to explain the reason behind the success or failure of each. We believe that applying more developed adversarial training methods (such as \cite{wgan}) can further enhance unsupervised domain adaptation. However, we witnessed that the method described in \cite{adda} requires excessive computing power to achieve acceptable results on rather basic tasks. Therefore, if the ideal that unsupervised domain adaptation pursues is to enable AI models to adapt themselves to new environments automatically, we believe that one direction of more research in the future can constitute online, computationally-efficient unsupervised domain adaptation algorithms.
	\bibliographystyle{ieeetr}
	\bibliography{citations}
\end{document}